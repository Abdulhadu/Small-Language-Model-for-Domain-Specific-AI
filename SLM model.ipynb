{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98c20cb-5f7c-439d-b973-539c4ad818f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (2.5.1+cpu)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'D:\\\\Cortech\\\\Learning\\\\youtube-stuffs\\\\env\\\\Lib\\\\site-packages\\\\matplotlib-3.10.0.dist-info\\\\RECORD2o9zp8ye.tmp' -> 'D:\\\\Cortech\\\\Learning\\\\youtube-stuffs\\\\env\\\\Lib\\\\site-packages\\\\matplotlib-3.10.0.dist-info\\\\RECORD'\n",
      "Check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: torchvision in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (0.20.1+cpu)\n",
      "Requirement already satisfied: torchaudio in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (2.5.1+cpu)\n",
      "Requirement already satisfied: numpy in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: transformers in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (4.48.1)\n",
      "Requirement already satisfied: tokenizers in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (0.21.0)\n",
      "Collecting torchtext\n",
      "  Downloading torchtext-0.18.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pypdf in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: langchain in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (0.3.15)\n",
      "Requirement already satisfied: filelock in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.55.4-cp311-cp311-win_amd64.whl.metadata (169 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from langchain) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from langchain) (3.11.11)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.31 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from langchain) (0.3.31)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from langchain) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from langchain) (0.2.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from langchain) (2.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.31->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: colorama in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: anyio in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.31->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Downloading torchtext-0.18.0-cp311-cp311-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 799.2 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 799.2 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 838.9 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/1.9 MB 838.4 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/1.9 MB 838.9 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/1.9 MB 838.9 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/1.9 MB 838.6 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.8/1.9 MB 846.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 850.4 kB/s eta 0:00:00\n",
      "Downloading matplotlib-3.10.0-cp311-cp311-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.0 MB 840.2 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.5/8.0 MB 840.2 kB/s eta 0:00:09\n",
      "   --- ------------------------------------ 0.8/8.0 MB 838.9 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.0/8.0 MB 825.2 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.0/8.0 MB 825.2 kB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.3/8.0 MB 838.9 kB/s eta 0:00:09\n",
      "   ------- -------------------------------- 1.6/8.0 MB 830.6 kB/s eta 0:00:08\n",
      "   ------- -------------------------------- 1.6/8.0 MB 830.6 kB/s eta 0:00:08\n",
      "   --------- ------------------------------ 1.8/8.0 MB 838.9 kB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.1/8.0 MB 839.1 kB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.1/8.0 MB 839.1 kB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 2.4/8.0 MB 838.9 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 2.6/8.0 MB 843.5 kB/s eta 0:00:07\n",
      "   -------------- ------------------------- 2.9/8.0 MB 843.2 kB/s eta 0:00:07\n",
      "   -------------- ------------------------- 2.9/8.0 MB 843.2 kB/s eta 0:00:07\n",
      "   --------------- ------------------------ 3.1/8.0 MB 842.7 kB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 3.4/8.0 MB 842.5 kB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 3.4/8.0 MB 842.5 kB/s eta 0:00:06\n",
      "   ------------------ --------------------- 3.7/8.0 MB 839.0 kB/s eta 0:00:06\n",
      "   ------------------ --------------------- 3.7/8.0 MB 839.0 kB/s eta 0:00:06\n",
      "   ------------------- -------------------- 3.9/8.0 MB 838.9 kB/s eta 0:00:05\n",
      "   -------------------- ------------------- 4.2/8.0 MB 841.6 kB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 4.5/8.0 MB 844.1 kB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 4.5/8.0 MB 844.1 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 4.7/8.0 MB 841.3 kB/s eta 0:00:04\n",
      "   ------------------------ --------------- 5.0/8.0 MB 841.3 kB/s eta 0:00:04\n",
      "   ------------------------ --------------- 5.0/8.0 MB 841.3 kB/s eta 0:00:04\n",
      "   -------------------------- ------------- 5.2/8.0 MB 841.1 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 5.5/8.0 MB 843.0 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 5.5/8.0 MB 843.0 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 5.8/8.0 MB 840.8 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 5.8/8.0 MB 840.8 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 6.0/8.0 MB 842.7 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 6.3/8.0 MB 840.7 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 6.3/8.0 MB 840.7 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 6.6/8.0 MB 840.7 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.8/8.0 MB 838.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.8/8.0 MB 838.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 7.1/8.0 MB 829.3 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 7.1/8.0 MB 829.3 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 7.3/8.0 MB 819.1 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/8.0 MB 819.1 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.0 MB 805.8 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.0 MB 805.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/8.0 MB 801.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 798.5 kB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp311-cp311-win_amd64.whl (219 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.4-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 598.5 kB/s eta 0:00:03\n",
      "   --------- ------------------------------ 0.5/2.2 MB 598.5 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.8/2.2 MB 633.2 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.0/2.2 MB 689.2 kB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.0/2.2 MB 689.2 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 713.8 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 735.9 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 735.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 751.1 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 751.1 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.2 MB 743.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 721.3 kB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp311-cp311-win_amd64.whl (71 kB)\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib, torchtext\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio numpy transformers tokenizers torchtext matplotlib pypdf langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc9e97e-24b5-470e-8365-9c83b3142f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'data/gpt4all.pdf', 'page': 0}, page_content='GPT4All: An Ecosystem of Open Source Compressed Language Models\\nYuvanesh Anand\\nNomic AI\\nyuvanesh@nomic.ai\\nZach Nussbaum\\nNomic AI\\nzach@nomic.ai\\nAdam Treat\\nNomic AI\\nadam@nomic.ai\\nAaron Miller\\nNomic AI\\naaron@nomic.ai\\nRichard Guo\\nNomic AI\\nrichard@nomic.ai\\nBen Schmidt\\nNomic AI\\nben@nomic.ai\\nGPT4All Community\\nPlanet Earth\\nBrandon Duderstadt∗\\nNomic AI\\nbrandon@nomic.ai\\nAndriy Mulyar∗\\nNomic AI\\nandriy@nomic.ai\\nAbstract\\nLarge language models (LLMs) have recently\\nachieved human-level performance on a range\\nof professional and academic benchmarks. The\\naccessibility of these models has lagged behind\\ntheir performance. State-of-the-art LLMs re-\\nquire costly infrastructure; are only accessible\\nvia rate-limited, geo-locked, and censored web\\ninterfaces; and lack publicly available code and\\ntechnical reports.\\nIn this paper, we tell the story of GPT4All, a\\npopular open source repository that aims to\\ndemocratize access to LLMs. We outline the\\ntechnical details of the original GPT4All model\\nfamily, as well as the evolution of the GPT4All\\nproject from a single model into a fully fledged\\nopen source ecosystem. It is our hope that\\nthis paper acts as both a technical overview of\\nthe original GPT4All models as well as a case\\nstudy on the subsequent growth of the GPT4All\\nopen source ecosystem.\\n1 Introduction\\nOn March 14 2023, OpenAI released GPT-4, a large\\nlanguage model capable of achieving human level per-\\nformance on a variety of professional and academic\\nbenchmarks. Despite the popularity of the release,\\nthe GPT-4 technical report (OpenAI, 2023) contained\\nvirtually no details regarding the architecture, hard-\\nware, training compute, dataset construction, or training\\nmethod used to create the model. Moreover, users could\\nonly access the model through the internet interface at\\nchat.openai.com, which was severely rate limited and\\nunavailable in several locales (e.g. Italy) (BBC News,\\n2023). Additionally, GPT-4 refused to answer a wide\\n∗ Shared Senior Authorship\\nvariety of queries, responding only with the now infa-\\nmous \"As an AI Language Model, I cannot...\" prefix\\n(Vincent, 2023). These transparency and accessibility\\nconcerns spurred several developers to begin creating\\nopen source large language model (LLM) alternatives.\\nSeveral grassroots efforts focused on fine tuning Meta’s\\nopen code LLaMA model (Touvron et al., 2023; McMil-\\nlan, 2023), whose weights were leaked on BitTorrent\\nless than a week prior to the release of GPT-4 (Verge,\\n2023). GPT4All started as one of these variants.\\nIn this paper, we tell the story of GPT4All. We com-\\nment on the technical details of the original GPT4All\\nmodel (Anand et al., 2023), as well as the evolution of\\nGPT4All from a single model to an ecosystem of several\\nmodels. We remark on the impact that the project has\\nhad on the open source community, and discuss future\\ndirections. It is our hope that this paper acts as both a\\ntechnical overview of the original GPT4All models as\\nwell as a case study on the subsequent growth of the\\nGPT4All open source ecosystem.\\n2 The Original GPT4All Model\\n2.1 Data Collection and Curation\\nTo train the original GPT4All model, we collected\\nroughly one million prompt-response pairs using the\\nGPT-3.5-Turbo OpenAI API between March 20, 2023\\nand March 26th, 2023. In particular, we gathered GPT-\\n3.5-Turbo responses to prompts of three publicly avail-\\nable datasets: the unified chip2 subset of LAION OIG,\\na random sub-sample of Stackoverflow Questions, and\\na sub-sample of Bigscience/P3 (Sanh et al., 2021). Fol-\\nlowing the approach in Stanford Alpaca (Taori et al.,\\n2023), an open source LLaMA variant that came just be-\\nfore GPT4All, we focused substantial effort on dataset\\ncuration.\\nThe collected dataset was loaded into Atlas (AI,\\n2023)—a visual interface for exploring and tagging mas-\\nsive unstructured datasets —for data curation. Using At-\\narXiv:2311.04931v1  [cs.CL]  6 Nov 2023')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the PDF document\n",
    "loader = PyPDFLoader('data/gpt4all.pdf')\n",
    "doc = loader.load_and_split()\n",
    "\n",
    "# Check the number of documents and preview the first one\n",
    "print(\"Number of documents:\", len(doc))\n",
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53eb8cc3-c50b-4a72-8c91-4ed98b26bd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT4All: An Ecosystem of Open Source Compressed Language Models\\nYuvanesh Anand\\nNomic AI\\nyuvanesh@nomic.ai\\nZach Nussbaum\\nNomic AI\\nzach@nomic.ai\\nAdam Treat\\nNomic AI\\nadam@nomic.ai\\nAaron Miller\\nNomic AI\\naaron@nomic.ai\\nRichard Guo\\nNomic AI\\nrichard@nomic.ai\\nBen Schmidt\\nNomic AI\\nben@nomic.ai\\nGPT4All Community\\nPlanet Earth\\nBrandon Duderstadt∗\\nNomic AI\\nbrandon@nomic.ai\\nAndriy Mulyar∗\\nNomic AI\\nandriy@nomic.ai\\nAbstract\\nLarge language models (LLMs) have recently\\nachieved human-level performance on a range\\nof professional and academic benchmarks. The\\naccessibility of these models has lagged behind\\ntheir performance. State-of-the-art LLMs re-\\nquire costly infrastructure; are only accessible\\nvia rate-limited, geo-locked, and censored web\\ninterfaces; and lack publicly available code and\\ntechnical reports.\\nIn this paper, we tell the story of GPT4All, a\\npopular open source repository that aims to\\ndemocratize access to LLMs. We outline the\\ntechnical details of the original GPT4All model\\nfamily, as well as the evolution of the GPT4All\\nproject from a single model into a fully fledged\\nopen source ecosystem. It is our hope that\\nthis paper acts as both a technical overview of\\nthe original GPT4All models as well as a case\\nstudy on the subsequent growth of the GPT4All\\nopen source ecosystem.\\n1 Introduction\\nOn March 14 2023, OpenAI released GPT-4, a large\\nlanguage model capable of achieving human level per-\\nformance on a variety of professional and academic\\nbenchmarks. Despite the popularity of the release,\\nthe GPT-4 technical report (OpenAI, 2023) contained\\nvirtually no details regarding the architecture, hard-\\nware, training compute, dataset construction, or training\\nmethod used to create the model. Moreover, users could\\nonly access the model through the internet interface at\\nchat.openai.com, which was severely rate limited and\\nunavailable in several locales (e.g. Italy) (BBC News,\\n2023). Additionally, GPT-4 refused to answer a wide\\n∗ Shared Senior Authorship\\nvariety of queries, responding only with the now infa-\\nmous \"As an AI Language Model, I cannot...\" prefix\\n(Vincent, 2023). These transparency and accessibility\\nconcerns spurred several developers to begin creating\\nopen source large language model (LLM) alternatives.\\nSeveral grassroots efforts focused on fine tuning Meta’s\\nopen code LLaMA model (Touvron et al., 2023; McMil-\\nlan, 2023), whose weights were leaked on BitTorrent\\nless than a week prior to the release of GPT-4 (Verge,\\n2023). GPT4All started as one of these variants.\\nIn this paper, we tell the story of GPT4All. We com-\\nment on the technical details of the original GPT4All\\nmodel (Anand et al., 2023), as well as the evolution of\\nGPT4All from a single model to an ecosystem of several\\nmodels. We remark on the impact that the project has\\nhad on the open source community, and discuss future\\ndirections. It is our hope that this paper acts as both a\\ntechnical overview of the original GPT4All models as\\nwell as a case study on the subsequent growth of the\\nGPT4All open source ecosystem.\\n2 The Original GPT4All Model\\n2.1 Data Collection and Curation\\nTo train the original GPT4All model, we collected\\nroughly one million prompt-response pairs using the\\nGPT-3.5-Turbo OpenAI API between March 20, 2023\\nand March 26th, 2023. In particular, we gathered GPT-\\n3.5-Turbo responses to prompts of three publicly avail-\\nable datasets: the unified chip2 subset of LAION OIG,\\na random sub-sample of Stackoverflow Questions, and\\na sub-sample of Bigscience/P3 (Sanh et al., 2021). Fol-\\nlowing the approach in Stanford Alpaca (Taori et al.,\\n2023), an open source LLaMA variant that came just be-\\nfore GPT4All, we focused substantial effort on dataset\\ncuration.\\nThe collected dataset was loaded into Atlas (AI,\\n2023)—a visual interface for exploring and tagging mas-\\nsive unstructured datasets —for data curation. Using At-\\narXiv:2311.04931v1  [cs.CL]  6 Nov 2023', 'las, we identified and removed subsets of the data where\\nGPT-3.5-Turbo refused to respond, had malformed out-\\nput, or produced a very short response. This resulted in\\nthe removal of the entire Bigscience/P3 subset of our\\ndata, as many P3 prompts induced responses that were\\nsimply one word. After curation, we were left with a set\\nof 437,605 prompt-response pairs, which we visualize\\nin Figure 1a.\\n2.2 Model Training\\nThe original GPT4All model was a fine tuned variant\\nof LLaMA 7B. In order to train it more efficiently, we\\nfroze the base weights of LLaMA, and only trained a\\nsmall set of LoRA (Hu et al., 2021) weights during the\\nfine tuning process. Detailed model hyper-parameters\\nand training code can be found in our associated code\\nrepository1.\\n2.3 Model Access\\nWe publicly released all data, training code, and model\\nweights for the community to build upon. Further, we\\nprovided a 4-bit quantized version of the model, which\\nenabled users to run it on their own commodity hard-\\nware without transferring data to a 3rd party service.\\nOur research and development costs were dominated\\nby ∼$800 in GPU spend (rented from Lambda Labs and\\nPaperspace) and ∼$500 in OpenAI API spend. Our final\\nGPT4All model could be trained in about eight hours\\non a Lambda Labs DGX A100 8x 80GB for a total cost\\nof ∼$100.\\n2.4 Model Evaluation\\nWe performed a preliminary evaluation of our model\\nusing the human evaluation data from the Self Instruct\\npaper (Wang et al., 2023). We reported the ground truth\\nperplexity of our model against what was, to our knowl-\\nedge, the best openly available alpaca-lora model at the\\ntime, provided by user chainyo on HuggingFace. Both\\nmodels had very large perplexities on a small number of\\ntasks, so we reported perplexities clipped to a maximum\\nof 100. We found that GPT4All produces stochastically\\nlower ground truth perplexities than alpaca-lora (Anand\\net al., 2023).\\n3 From a Model to an Ecosystem\\n3.1 GPT4All-J: Repository Growth and the\\nimplications of the LLaMA License\\nThe GPT4All repository grew rapidly after its release,\\ngaining over 20000 GitHub stars in just one week, as\\nshown in Figure 2. This growth was supported by an\\nin-person hackathon hosted in New York City three days\\nafter the model release, which attracted several hundred\\nparticipants. As the Nomic discord, the home of online\\ndiscussion about GPT4All, ballooned to over 10000\\npeople, one thing became very clear - there was massive\\ndemand for a model that could be used commercially.\\n1https://github.com/nomic-ai/gpt4all\\nThe LLaMA model that GPT4All was based on was\\nlicensed for research only, which severely limited the\\nset of domains that GPT4All could be applied in. As\\na response to this, the Nomic team repeated the model\\ntraining procedure of the original GPT4All model, but\\nbased on the already open source and commercially li-\\ncensed GPT-J model (Wang and Komatsuzaki, 2021).\\nGPT4All-J also had an augmented training set, which\\ncontained multi-turn QA examples and creative writing\\nsuch as poetry, rap, and short stories. The creative writ-\\ning prompts were generated by filling in schemas such\\nas \"Write a [CREATIVE STORY TYPE] about [NOUN]\\nin the style of [PERSON].\" We again employed Atlas\\nto curate the prompt-response pairs in this data set.\\nOur evaluation methodology also evolved as the\\nproject grew. In particular, we began evaluating\\nGPT4All models using a suite of seven reasoning\\ntasks that were used for evaluation of the Databricks\\nDolly (Conover et al., 2023b) model, which was re-\\nleased on April 12, 2023. Unfortunately, GPT4All-J did\\nnot outperform other prominent open source models on\\nthis evaluation. As a result, we endeavoured to create a\\nmodel that did.\\n3.2 GPT4All-Snoozy: the Emergence of the\\nGPT4All Ecosystem\\nGPT4All-Snoozy was developed using roughly the same\\nprocedure as the previous GPT4All models, but with a\\nfew key modifications. First, GPT4All-Snoozy used the\\nLLaMA-13B base model due to its superior base metrics', 'procedure as the previous GPT4All models, but with a\\nfew key modifications. First, GPT4All-Snoozy used the\\nLLaMA-13B base model due to its superior base metrics\\nwhen compared to GPT-J. Next, GPT4All-Snoozy incor-\\nporated the Dolly’s training data into its train mix. After\\ndata curation and deduplication with Atlas, this yielded\\na training set of 739,259 total prompt-response pairs.\\nWe dubbed the model that resulted from training on this\\nimproved dataset GPT4All-Snoozy. As shown in Figure\\n1, GPT4All-Snoozy had the best average score on our\\nevaluation benchmark of any model in the ecosystem at\\nthe time of its release.\\nConcurrently with the development of GPT4All, sev-\\neral organizations such as LMSys, Stability AI, BAIR,\\nand Databricks built and deployed open source language\\nmodels. We heard increasingly from the community that\\nthey wanted quantized versions of these models for local\\nuse. As we realized that organizations with ever more\\nresources were developing source language models, we\\ndecided to pivot our effort away from training increas-\\ningly capable models and towards providing easy access\\nto the plethora of models being produced by the open\\nsource community. Practically, this meant spending our\\ntime compressing open source models for use on com-\\nmodity hardware, providing stable and simple high level\\nmodel APIs, and supporting a GUI for no code model\\nexperimentation.\\n3.3 The Current State of GPT4All\\nToday, GPT4All is focused on improving the accessi-\\nbility of open source language models. The repository', '(a)\\n (b)\\n (c)\\n (d)\\nFigure 1: TSNE visualizations showing the progression of the GPT4All train set. Panel (a) shows the original\\nuncurated data. The red arrow denotes a region of highly homogeneous prompt-response pairs. The coloring denotes\\nwhich open dataset contributed the prompt. Panel (b) shows the original GPT4All data after curation. This panel,\\nas well as panels (c) and (d) are 10 colored by topic, which Atlas automatically extracts. Notice that the large\\nhomogeneous prompt-response blobs no longer appearl. Panel (c) shows the GPT4All-J dataset. The \"starburst\"\\nclusters introduced on the right side of the panel correspond to the newly added creative data. Panel (d) shows\\nthe final GPT4All-snoozy dataset. All datasets have been released to the public, and can be interactively explored\\nonline. In the web version of this article, you can click on a panel to be taken to its interactive visualization.\\nModel BoolQ PIQA HellaSwag WinoG. ARC-e ARC-c OBQA Avg.\\nGPT4All-J 6B v1.0* 73.4 74.8 63.4 64.7 54.9 36 40.2 58.2\\nGPT4All-J v1.1-breezy* 74 75.1 63.2 63.6 55.4 34.9 38.4 57.8\\nGPT4All-J v1.2-jazzy* 74.8 74.9 63.6 63.8 56.6 35.3 41 58.6\\nGPT4All-J v1.3-groovy* 73.6 74.3 63.8 63.5 57.7 35 38.8 58.1\\nGPT4All-J Lora 6B* 68.6 75.8 66.2 63.5 56.4 35.7 40.2 58.1\\nGPT4All LLaMa Lora 7B* 73.1 77.6 72.1 67.8 51.1 40.4 40.2 60.3\\nGPT4All 13B snoozy* 83.3 79.2 75 71.3 60.9 44.2 43.4 65.3\\nGPT4All Falcon 77.6 79.8 74.9 70.1 67.9 43.4 42.6 65.2\\nNous-Hermes (Nous-Research, 2023b) 79.5 78.9 80 71.9 74.2 50.9 46.4 68.8\\nNous-Hermes2 (Nous-Research, 2023c) 83.9 80.7 80.1 71.3 75.7 52.1 46.2 70.0\\nNous-Puffin (Nous-Research, 2023d) 81.5 80.7 80.4 72.5 77.6 50.7 45.6 69.9\\nDolly 6B* (Conover et al., 2023a) 68.8 77.3 67.6 63.9 62.9 38.7 41.2 60.1\\nDolly 12B* (Conover et al., 2023b) 56.7 75.4 71 62.2 64.6 38.5 40.4 58.4\\nAlpaca 7B* (Taori et al., 2023) 73.9 77.2 73.9 66.1 59.8 43.3 43.4 62.5\\nAlpaca Lora 7B* (Wang, 2023) 74.3 79.3 74 68.8 56.6 43.9 42.6 62.8\\nGPT-J* 6.7B (Wang and Komatsuzaki, 2021) 65.4 76.2 66.2 64.1 62.2 36.6 38.2 58.4\\nLLama 7B* (Touvron et al., 2023) 73.1 77.4 73 66.9 52.5 41.4 42.4 61.0\\nLLama 13B* (Touvron et al., 2023) 68.5 79.1 76.2 70.1 60 44.6 42.2 63.0\\nPythia 6.7B* (Biderman et al., 2023) 63.5 76.3 64 61.1 61.3 35.2 37.2 56.9\\nPythia 12B* (Biderman et al., 2023) 67.7 76.6 67.3 63.8 63.9 34.8 38 58.9\\nFastchat T5* (Zheng et al., 2023) 81.5 64.6 46.3 61.8 49.3 33.3 39.4 53.7\\nFastchat Vicuña* 7B (Zheng et al., 2023) 76.6 77.2 70.7 67.3 53.5 41.2 40.8 61.0\\nFastchat Vicuña 13B* (Zheng et al., 2023) 81.5 76.8 73.3 66.7 57.4 42.7 43.6 63.1\\nStableVicuña RLHF* (Stability-AI, 2023) 82.3 78.6 74.1 70.9 61 43.5 44.4 65.0\\nStableLM Tuned* (Stability-AI, 2023) 62.5 71.2 53.6 54.8 52.4 31.1 33.4 51.3\\nStableLM Base* (Stability-AI, 2023) 60.1 67.4 41.2 50.1 44.9 27 32 46.1\\nKoala 13B* (Geng et al., 2023) 76.5 77.9 72.6 68.8 54.3 41 42.8 62.0\\nOpen Assistant Pythia 12B* 67.9 78 68.1 65 64.2 40.4 43.2 61.0\\nMosaic MPT7B (MosaicML-Team, 2023) 74.8 79.3 76.3 68.6 70 42.2 42.6 64.8\\nMosaic mpt-instruct (MosaicML-Team, 2023) 74.3 80.4 77.2 67.8 72.2 44.6 43 65.6\\nMosaic mpt-chat (MosaicML-Team, 2023) 77.1 78.2 74.5 67.5 69.4 43.3 44.2 64.9\\nWizard 7B (Xu et al., 2023) 78.4 77.2 69.9 66.5 56.8 40.5 42.6 61.7\\nWizard 7B Uncensored (Xu et al., 2023) 77.7 74.2 68 65.2 53.5 38.7 41.6 59.8\\nWizard 13B Uncensored (Xu et al., 2023) 78.4 75.5 72.1 69.5 57.5 40.4 44 62.5\\nGPT4-x-Vicuna-13b (Nous-Research, 2023a) 81.3 75 75.2 65 58.7 43.9 43.6 63.2\\nFalcon 7b (Almazrouei et al., 2023) 73.6 80.7 76.3 67.3 71 43.3 44.4 65.2\\nFalcon 7b instruct (Almazrouei et al., 2023) 70.9 78.6 69.8 66.7 67.9 42.7 41.2 62.5\\ntext-davinci-003 88.1 83.8 83.4 75.8 83.9 63.9 51.0 75.7\\nTable 1: Evaluations of all language models in the GPT4All ecosystem as of August 1, 2023. Code models are not\\nincluded. OpenAI’s text-davinci-003 is included as a point of comparison. The best overall performing model in the', 'included. OpenAI’s text-davinci-003 is included as a point of comparison. The best overall performing model in the\\nGPT4All ecosystem, Nous-Hermes2, achieves over 92% of the average performance of text-davinci-003. Models\\nmarked with an asterisk were available in the ecosystem as of the release of GPT4All-Snoozy. Note that at release,\\nGPT4All-Snoozy had the best average performance of any model in the ecosystem. Bolded numbers indicate the\\nbest performing model as of August 1, 2023.', 'Figure 2: Comparison of the github start growth of GPT4All, Meta’s LLaMA, and Stanford’s Alpaca. We conjecture\\nthat GPT4All achieved and maintains faster ecosystem growth due to the focus on access, which allows more users\\nto meaningfully participate.\\nprovides compressed versions of open source models\\nfor use on commodity hardware, stable and simple high\\nlevel model APIs, and a GUI for no code model ex-\\nperimentation. The project continues to increase in\\npopularity, and as of August 1 2023, has garnered over\\n50000 GitHub stars and over 5000 forks.\\nGPT4All currently provides native support and\\nbenchmark data for over 35 models (see Figure 1), and\\nincludes several models co-developed with industry part-\\nners such as Replit and Hugging Face. GPT4All also\\nprovides high level model APIs in languages includ-\\ning Python, Typescript, Go, C#, and Java, among oth-\\ners. Furthermore, the GPT4All no code GUI currently\\nsupports the workflows of over 50000 monthly active\\nusers, with over 25% of users coming back to the tool\\nevery day of the week. (Note that all GPT4All user\\ndata is collected on an opt inbasis.) GPT4All has be-\\ncome the top language model integration in the popular\\nopen source AI orchestration library LangChain (Chase,\\n2022), and powers many popular open source projects\\nsuch as PrivateGPT (imartinez, 2023), Quiver (StanGi-\\nrard, 2023), and MindsDB (MindsDB, 2023), among\\nothers. GPT4All is the 3rd fastest growing GitHub\\nrepository of all time (Leo, 2023), and is the 185th most\\npopular repository on the platform, by star count.\\n4 The Future of GPT4All\\nIn the future, we will continue to grow GPT4All, sup-\\nporting it as the de facto solution for LLM accessibil-\\nity. Concretely, this means continuing to compress and\\ndistribute important open-source language models de-\\nveloped by the community, as well as compressing and\\ndistributing increasingly multimodal AI models. Fur-\\nthermore, we will expand the set of hardware devices\\nthat GPT4All models run on, so that GPT4All models\\n“just work\" on any machine, whether it comes equipped\\nwith Apple Metal silicon, NVIDIA, AMD, or other edge-\\naccelerated hardware. Overall, we envision a world\\nwhere anyone, anywhere, with any machine, can access\\nand contribute to the cutting edge of AI.\\nLimitations\\nBy enabling access to large language models, the\\nGPT4All project also inherits many of the ethical con-\\ncerns associated with generative models. Principal\\namong these is the concern that unfiltered language\\nmodels like GPT4All enable malicious users to generate\\ncontent that could be harmful and dangerous (e.g., in-\\nstructions on building bioweapons). While we recognize\\nthis risk, we also acknowledge the risk of concentrating\\nthis technology in the hands of a limited number of in-\\ncreasingly secretive research groups. We believe that\\nthe risk of focusing on the benefits of language model\\ntechnology significantly outweighs the risk of misuse,\\nand hence we prefer to make the technology as widely\\navailable as possible.\\nFinally, we realize the challenge in assigning credit\\nfor large-scale open source initiatives. We make a first\\nattempt at fair credit assignment by explicitly includ-\\ning the GPT4All open source developers as authors on\\nthis work, but recognize that this is insufficient fully\\ncharacterize everyone involved in the GPT4All effort.\\nFurthermore, we acknowledge the difficulty in citing\\nopen source works that do not necessarily have standard-\\nized citations, and do our best in this paper to provide\\nURLs to projects whenever possible. We encourage\\nfurther research in the area of open source credit as-\\nsignment, and hope to be able to support some of this\\nresearch ourselves in the future.', 'References\\nNomic AI. 2023. Atlas. https://atlas.nomic.ai/.\\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\\nMerouane Debbah, Etienne Goffinet, Daniel Hes-\\nlow, Julien Launay, Quentin Malartic, Badreddine\\nNoune, Baptiste Pannier, and Guilherme Penedo.\\n2023. Falcon-40B: an open large language model\\nwith state-of-the-art performance.\\nYuvanesh Anand, Zach Nussbaum, Brandon Duder-\\nstadt, Benjamin Schmidt, and Andriy Mulyar. 2023.\\nGpt4all: Training an assistant-style chatbot with\\nlarge scale data distillation from gpt-3.5-turbo.\\nhttps://github.com/nomic-ai/gpt4all.\\nBBC News. 2023. Chatgpt banned in italy over privacy\\nconcerns. BBC News.\\nStella Biderman, Hailey Schoelkopf, Quentin An-\\nthony, Herbie Bradley, Kyle O’Brien, Eric Hal-\\nlahan, Mohammad Aflah Khan, Shivanshu Puro-\\nhit, USVSN Sai Prashanth, Edward Raff, Aviya\\nSkowron, Lintang Sutawika, and Oskar van der Wal.\\n2023. Pythia: A suite for analyzing large language\\nmodels across training and scaling.\\nHarrison Chase. 2022. langchain. https://github.\\ncom/langchain-ai/langchain.\\nMike Conover, Matt Hayes, Ankit Mathur, Xiangrui\\nMeng, Jianwei Xie, Jun Wan, Ali Ghodsi, Patrick\\nWendell, and Matei Zaharia. 2023a. Hello dolly:\\nDemocratizing the magic of chatgpt with open mod-\\nels.\\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,\\nMatei Zaharia, and Reynold Xin. 2023b. Free dolly:\\nIntroducing the world’s first truly open instruction-\\ntuned llm.\\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal-\\nlace, Pieter Abbeel, Sergey Levine, and Dawn Song.\\n2023. Koala: A dialogue model for academic re-\\nsearch. Blog post.\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\\nlarge language models.\\nimartinez. 2023. privategpt. https://github.com/\\nimartinez/privateGPT.\\nOscar Leo. 2023. GitHub: The Fastest Growing Repos-\\nitories of All Time.\\nRobert McMillan. 2023. A meta platforms leak put\\npowerful ai in the hands of everyone. The Wall\\nStreet Journal.\\nMindsDB. 2023. Mindsdb. https://github.com/\\nmindsdb/mindsdb. GitHub repository.\\nMosaicML-Team. 2023. Introducing mpt-7b: A new\\nstandard for open-source, commercially usable llms.\\nAccessed: 2023-08-07.\\nNous-Research. 2023a. gpt4-x-vicuna-13b.\\nhttps://huggingface.co/NousResearch/\\ngpt4-x-vicuna-13b . Model on Hugging Face.\\nNous-Research. 2023b. Nous-hermes-13b.\\nhttps://huggingface.co/NousResearch/\\nNous-Hermes-13b. Model on Hugging Face.\\nNous-Research. 2023c. Nous-hermes-llama-2-7b.\\nhttps://huggingface.co/NousResearch/\\nNous-Hermes-llama-2-7b . Model on Hugging\\nFace.\\nNous-Research. 2023d. Redmond-puffin-13b.\\nhttps://huggingface.co/NousResearch/\\nRedmond-Puffin-13B. Model on Hugging Face.\\nOpenAI. 2023. Gpt-4 technical report.\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak,\\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian\\nJiang, Han Wang, Matteo Manica, Sheng Shen,\\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\\nheesht Sharma, Andrea Santilli, Thibault Fevry, Ja-\\nson Alan Fries, Ryan Teehan, Stella Biderman, Leo\\nGao, Tali Bers, Thomas Wolf, and Alexander M.\\nRush. 2021. Multitask prompted training enables\\nzero-shot task generalization.\\nStability-AI. 2023. Stablelm. https://github.com/\\nStability-AI/StableLM. GitHub repository.\\nStanGirard. 2023. quivr. https://github.com/\\nStanGirard/quivr. GitHub repository.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model. https://\\ngithub.com/tatsu-lab/stanford_alpaca.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier', 'and Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model. https://\\ngithub.com/tatsu-lab/stanford_alpaca.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro,\\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin,\\nEdouard Grave, and Guillaume Lample. 2023.\\nLlama: Open and efficient foundation language\\nmodels.\\nThe Verge. 2023. Meta’s powerful ai language model\\nhas leaked online — what happens now? The Verge.\\nJames Vincent. 2023. As an ai generated language\\nmodel: The phrase that shows how ai is polluting\\nthe web. The Verge.', 'Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B:\\nA 6 Billion Parameter Autoregressive Language\\nModel. https://github.com/kingoflolz/\\nmesh-transformer-jax.\\nEric J. Wang. 2023. alpaca-lora. https://github.\\ncom/tloen/alpaca-lora. GitHub repository.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\\nLiu, Noah A. Smith, Daniel Khashabi, and Han-\\nnaneh Hajishirzi. 2023. Self-instruct: Aligning lan-\\nguage models with self-generated instructions.\\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\\nJiang. 2023. Wizardlm: Empowering large language\\nmodels to follow complex instructions.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\\nllm-as-a-judge with mt-bench and chatbot arena.']\n",
      "tokenizer trained\n",
      "length of tokenizer 2862\n",
      "Encoded sample: [124, 21, 233, 726, 108, 308, 1567, 2345, 891, 1179, 1275, 682, 283, 159, 1840, 24, 289, 9, 140, 1000, 1326, 283, 159, 1845, 24, 289, 9, 140, 2407, 2655, 283, 159, 2066, 24, 289, 9, 140, 1411, 2427, 283, 159, 54]\n",
      "Decoded sample: GPT4All : An Ecosystem of Open Source Compressed Language Models Yuvanesh Anand Nomic AI yuvanesh @ nomic . ai Zach Nussbaum Nomic AI zach @ nomic . ai Adam Treat Nomic AI adam @ nomic . ai Aaron Miller Nomic AI a\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Initialize tokenizer\n",
    "vocab_size = 5000\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "# Train tokenizer on the text from the document\n",
    "trainer = BpeTrainer(vocab_size = vocab_size-5)\n",
    "special_token_dict = {\n",
    "    \"<start>\" : 0,\n",
    "    \"<|end|>\": 1,\n",
    "    \"<|system|>\": 2, \n",
    "    \"<|user|>\": 3,\n",
    "    \"<|ai|>\": 4\n",
    "}\n",
    "tokenizer.add_special_tokens(list(special_token_dict))\n",
    "# trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<start>\", \"<|end|>\", \"<system>\", \"<|user|>\", \"<|ai|>\"])\n",
    "text_data = [doc_chunk.page_content for doc_chunk in doc]  # Collect text from all chunks\n",
    "print(text_data)\n",
    "text_list = [text_data]\n",
    "tokenizer.train_from_iterator(text_list, trainer)\n",
    "\n",
    "# Save and reload tokenizer (optional)\n",
    "print(\"tokenizer trained\")\n",
    "print(\"length of tokenizer\", len(tokenizer.get_vocab()))\n",
    "# tokenizer.save(\"tokenizer.json\")\n",
    "# tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "\n",
    "# Define encode and decode functions\n",
    "def encode(text):\n",
    "    return tokenizer.encode(text).ids\n",
    "\n",
    "def decode(ids):\n",
    "    return tokenizer.decode(ids)\n",
    "\n",
    "# Test the tokenizer\n",
    "sample_text = text_data[0][:200]  # Use the first 200 characters of the first document chunk\n",
    "encoded = encode(sample_text)\n",
    "decoded = decode(encoded)\n",
    "\n",
    "print(\"Encoded sample:\", encoded)\n",
    "print(\"Decoded sample:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b4ee566-9559-49fe-a92b-f9f3f1b06267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4All: An Ecosystem of Open Source Compressed Language Models\n",
      "Yuvanesh Anand\n",
      "Nomic AI\n",
      "yuvanesh@nomic.ai\n",
      "Zach Nussbaum\n",
      "Nomic AI\n",
      "zach@nomic.ai\n",
      "Adam Treat\n",
      "Nomic AI\n",
      "adam@nomic.ai\n",
      "Aaron Miller\n",
      "Nomic AI\n",
      "aaron@nomic.ai\n",
      "Richard Guo\n",
      "Nomic AI\n",
      "richard@nomic.ai\n",
      "Ben Schmidt\n",
      "Nomic AI\n",
      "ben@nomic.ai\n",
      "GPT4All Community\n",
      "Planet Earth\n",
      "Brandon Duderstadt∗\n",
      "Nomic AI\n",
      "brandon@nomic.ai\n",
      "Andriy Mulyar∗\n",
      "Nomic AI\n",
      "andriy@nomic.ai\n",
      "Abstract\n",
      "Large language models (LLMs) have recently\n",
      "achieved human-level performance on a range\n",
      "of professional and academic benchmarks. The\n",
      "accessibility of these models has lagged behind\n",
      "their performance. State-of-the-art LLMs re-\n",
      "quire costly infrastructure; are only accessible\n",
      "via rate-limited, geo-locked, and censored web\n",
      "interfaces; and lack publicly available code and\n",
      "technical reports.\n",
      "In this paper, we tell the story of GPT4All, a\n",
      "popular open source repository that aims to\n",
      "democratize access to LLMs. We outline the\n",
      "technical details of the original GPT4All model\n",
      "family, as well as the evolution of the GPT4All\n",
      "project from a single model into a fully fledged\n",
      "open source ecosystem. It is our hope that\n",
      "this paper acts as both a technical overview of\n",
      "the original GPT4All models as well as a case\n",
      "study on the subsequent growth of the GPT4All\n",
      "open source ecosystem.\n",
      "1 Introduction\n",
      "On March 14 2023, OpenAI released GPT-4, a large\n",
      "language model capable of achieving human level per-\n",
      "formance on a variety of professional and academic\n",
      "benchmarks. Despite the popularity of the release,\n",
      "the GPT-4 technical report (OpenAI, 2023) contained\n",
      "virtually no details regarding the architecture, hard-\n",
      "ware, training compute, dataset construction, or training\n",
      "method used to create the model. Moreover, users could\n",
      "only access the model through the internet interface at\n",
      "chat.openai.com, which was severely rate limited and\n",
      "unavailable in several locales (e.g. Italy) (BBC News,\n",
      "2023). Additionally, GPT-4 refused to answer a wide\n",
      "∗ Shared Senior Authorship\n",
      "variety of queries, responding only with the now infa-\n",
      "mous \"As an AI Language Model, I cannot...\" prefix\n",
      "(Vincent, 2023). These transparency and accessibility\n",
      "concerns spurred several developers to begin creating\n",
      "open source large language model (LLM) alternatives.\n",
      "Several grassroots efforts focused on fine tuning Meta’s\n",
      "open code LLaMA model (Touvron et al., 2023; McMil-\n",
      "lan, 2023), whose weights were leaked on BitTorrent\n",
      "less than a week prior to the release of GPT-4 (Verge,\n",
      "2023). GPT4All started as one of these variants.\n",
      "In this paper, we tell the story of GPT4All. We com-\n",
      "ment on the technical details of the original GPT4All\n",
      "model (Anand et al., 2023), as well as the evolution of\n",
      "GPT4All from a single model to an ecosystem of several\n",
      "models. We remark on the impact that the project has\n",
      "had on the open source community, and discuss future\n",
      "directions. It is our hope that this paper acts as both a\n",
      "technical overview of the original GPT4All models as\n",
      "well as a case study on the subsequent growth of the\n",
      "GPT4All open source ecosystem.\n",
      "2 The Original GPT4All Model\n",
      "2.1 Data Collection and Curation\n",
      "To train the original GPT4All model, we collected\n",
      "roughly one million prompt-response pairs using the\n",
      "GPT-3.5-Turbo OpenAI API between March 20, 2023\n",
      "and March 26th, 2023. In particular, we gathered GPT-\n",
      "3.5-Turbo responses to prompts of three publicly avail-\n",
      "able datasets: the unified chip2 subset of LAION OIG,\n",
      "a random sub-sample of Stackoverflow Questions, and\n",
      "a sub-sample of Bigscience/P3 (Sanh et al., 2021). Fol-\n",
      "lowing the approach in Stanford Alpaca (Taori et al.,\n",
      "2023), an open source LLaMA variant that came just be-\n",
      "fore GPT4All, we focused substantial effort on dataset\n",
      "curation.\n",
      "The collected dataset was loaded into Atlas (AI,\n",
      "2023)—a visual interface for exploring and tagging mas-\n",
      "sive unstructured datasets —for data curation. Using At-\n",
      "arXiv:2311.04931v1  [cs.CL]  6 Nov 2023 las, we identified and removed subsets of the data where\n",
      "GPT-3.5-Turbo refused to respond, had malformed out-\n",
      "put, or produced a very short response. This resulted in\n",
      "the removal of the entire Bigscience/P3 subset of our\n",
      "data, as many P3 prompts induced responses that were\n",
      "simply one word. After curation, we were left with a set\n",
      "of 437,605 prompt-response pairs, which we visualize\n",
      "in Figure 1a.\n",
      "2.2 Model Training\n",
      "The original GPT4All model was a fine tuned variant\n",
      "of LLaMA 7B. In order to train it more efficiently, we\n",
      "froze the base weights of LLaMA, and only trained a\n",
      "small set of LoRA (Hu et al., 2021) weights during the\n",
      "fine tuning process. Detailed model hyper-parameters\n",
      "and training code can be found in our associated code\n",
      "repository1.\n",
      "2.3 Model Access\n",
      "We publicly released all data, training code, and model\n",
      "weights for the community to build upon. Further, we\n",
      "provided a 4-bit quantized version of the model, which\n",
      "enabled users to run it on their own commodity hard-\n",
      "ware without transferring data to a 3rd party service.\n",
      "Our research and development costs were dominated\n",
      "by ∼$800 in GPU spend (rented from Lambda Labs and\n",
      "Paperspace) and ∼$500 in OpenAI API spend. Our final\n",
      "GPT4All model could be trained in about eight hours\n",
      "on a Lambda Labs DGX A100 8x 80GB for a total cost\n",
      "of ∼$100.\n",
      "2.4 Model Evaluation\n",
      "We performed a preliminary evaluation of our model\n",
      "using the human evaluation data from the Self Instruct\n",
      "paper (Wang et al., 2023). We reported the ground truth\n",
      "perplexity of our model against what was, to our knowl-\n",
      "edge, the best openly available alpaca-lora model at the\n",
      "time, provided by user chainyo on HuggingFace. Both\n",
      "models had very large perplexities on a small number of\n",
      "tasks, so we reported perplexities clipped to a maximum\n",
      "of 100. We found that GPT4All produces stochastically\n",
      "lower ground truth perplexities than alpaca-lora (Anand\n",
      "et al., 2023).\n",
      "3 From a Model to an Ecosystem\n",
      "3.1 GPT4All-J: Repository Growth and the\n",
      "implications of the LLaMA License\n",
      "The GPT4All repository grew rapidly after its release,\n",
      "gaining over 20000 GitHub stars in just one week, as\n",
      "shown in Figure 2. This growth was supported by an\n",
      "in-person hackathon hosted in New York City three days\n",
      "after the model release, which attracted several hundred\n",
      "participants. As the Nomic discord, the home of online\n",
      "discussion about GPT4All, ballooned to over 10000\n",
      "people, one thing became very clear - there was massive\n",
      "demand for a model that could be used commercially.\n",
      "1https://github.com/nomic-ai/gpt4all\n",
      "The LLaMA model that GPT4All was based on was\n",
      "licensed for research only, which severely limited the\n",
      "set of domains that GPT4All could be applied in. As\n",
      "a response to this, the Nomic team repeated the model\n",
      "training procedure of the original GPT4All model, but\n",
      "based on the already open source and commercially li-\n",
      "censed GPT-J model (Wang and Komatsuzaki, 2021).\n",
      "GPT4All-J also had an augmented training set, which\n",
      "contained multi-turn QA examples and creative writing\n",
      "such as poetry, rap, and short stories. The creative writ-\n",
      "ing prompts were generated by filling in schemas such\n",
      "as \"Write a [CREATIVE STORY TYPE] about [NOUN]\n",
      "in the style of [PERSON].\" We again employed Atlas\n",
      "to curate the prompt-response pairs in this data set.\n",
      "Our evaluation methodology also evolved as the\n",
      "project grew. In particular, we began evaluating\n",
      "GPT4All models using a suite of seven reasoning\n",
      "tasks that were used for evaluation of the Databricks\n",
      "Dolly (Conover et al., 2023b) model, which was re-\n",
      "leased on April 12, 2023. Unfortunately, GPT4All-J did\n",
      "not outperform other prominent open source models on\n",
      "this evaluation. As a result, we endeavoured to create a\n",
      "model that did.\n",
      "3.2 GPT4All-Snoozy: the Emergence of the\n",
      "GPT4All Ecosystem\n",
      "GPT4All-Snoozy was developed using roughly the same\n",
      "procedure as the previous GPT4All models, but with a\n",
      "few key modifications. First, GPT4All-Snoozy used the\n",
      "LLaMA-13B base model due to its superior base metrics procedure as the previous GPT4All models, but with a\n",
      "few key modifications. First, GPT4All-Snoozy used the\n",
      "LLaMA-13B base model due to its superior base metrics\n",
      "when compared to GPT-J. Next, GPT4All-Snoozy incor-\n",
      "porated the Dolly’s training data into its train mix. After\n",
      "data curation and deduplication with Atlas, this yielded\n",
      "a training set of 739,259 total prompt-response pairs.\n",
      "We dubbed the model that resulted from training on this\n",
      "improved dataset GPT4All-Snoozy. As shown in Figure\n",
      "1, GPT4All-Snoozy had the best average score on our\n",
      "evaluation benchmark of any model in the ecosystem at\n",
      "the time of its release.\n",
      "Concurrently with the development of GPT4All, sev-\n",
      "eral organizations such as LMSys, Stability AI, BAIR,\n",
      "and Databricks built and deployed open source language\n",
      "models. We heard increasingly from the community that\n",
      "they wanted quantized versions of these models for local\n",
      "use. As we realized that organizations with ever more\n",
      "resources were developing source language models, we\n",
      "decided to pivot our effort away from training increas-\n",
      "ingly capable models and towards providing easy access\n",
      "to the plethora of models being produced by the open\n",
      "source community. Practically, this meant spending our\n",
      "time compressing open source models for use on com-\n",
      "modity hardware, providing stable and simple high level\n",
      "model APIs, and supporting a GUI for no code model\n",
      "experimentation.\n",
      "3.3 The Current State of GPT4All\n",
      "Today, GPT4All is focused on improving the accessi-\n",
      "bility of open source language models. The repository (a)\n",
      " (b)\n",
      " (c)\n",
      " (d)\n",
      "Figure 1: TSNE visualizations showing the progression of the GPT4All train set. Panel (a) shows the original\n",
      "uncurated data. The red arrow denotes a region of highly homogeneous prompt-response pairs. The coloring denotes\n",
      "which open dataset contributed the prompt. Panel (b) shows the original GPT4All data after curation. This panel,\n",
      "as well as panels (c) and (d) are 10 colored by topic, which Atlas automatically extracts. Notice that the large\n",
      "homogeneous prompt-response blobs no longer appearl. Panel (c) shows the GPT4All-J dataset. The \"starburst\"\n",
      "clusters introduced on the right side of the panel correspond to the newly added creative data. Panel (d) shows\n",
      "the final GPT4All-snoozy dataset. All datasets have been released to the public, and can be interactively explored\n",
      "online. In the web version of this article, you can click on a panel to be taken to its interactive visualization.\n",
      "Model BoolQ PIQA HellaSwag WinoG. ARC-e ARC-c OBQA Avg.\n",
      "GPT4All-J 6B v1.0* 73.4 74.8 63.4 64.7 54.9 36 40.2 58.2\n",
      "GPT4All-J v1.1-breezy* 74 75.1 63.2 63.6 55.4 34.9 38.4 57.8\n",
      "GPT4All-J v1.2-jazzy* 74.8 74.9 63.6 63.8 56.6 35.3 41 58.6\n",
      "GPT4All-J v1.3-groovy* 73.6 74.3 63.8 63.5 57.7 35 38.8 58.1\n",
      "GPT4All-J Lora 6B* 68.6 75.8 66.2 63.5 56.4 35.7 40.2 58.1\n",
      "GPT4All LLaMa Lora 7B* 73.1 77.6 72.1 67.8 51.1 40.4 40.2 60.3\n",
      "GPT4All 13B snoozy* 83.3 79.2 75 71.3 60.9 44.2 43.4 65.3\n",
      "GPT4All Falcon 77.6 79.8 74.9 70.1 67.9 43.4 42.6 65.2\n",
      "Nous-Hermes (Nous-Research, 2023b) 79.5 78.9 80 71.9 74.2 50.9 46.4 68.8\n",
      "Nous-Hermes2 (Nous-Research, 2023c) 83.9 80.7 80.1 71.3 75.7 52.1 46.2 70.0\n",
      "Nous-Puffin (Nous-Research, 2023d) 81.5 80.7 80.4 72.5 77.6 50.7 45.6 69.9\n",
      "Dolly 6B* (Conover et al., 2023a) 68.8 77.3 67.6 63.9 62.9 38.7 41.2 60.1\n",
      "Dolly 12B* (Conover et al., 2023b) 56.7 75.4 71 62.2 64.6 38.5 40.4 58.4\n",
      "Alpaca 7B* (Taori et al., 2023) 73.9 77.2 73.9 66.1 59.8 43.3 43.4 62.5\n",
      "Alpaca Lora 7B* (Wang, 2023) 74.3 79.3 74 68.8 56.6 43.9 42.6 62.8\n",
      "GPT-J* 6.7B (Wang and Komatsuzaki, 2021) 65.4 76.2 66.2 64.1 62.2 36.6 38.2 58.4\n",
      "LLama 7B* (Touvron et al., 2023) 73.1 77.4 73 66.9 52.5 41.4 42.4 61.0\n",
      "LLama 13B* (Touvron et al., 2023) 68.5 79.1 76.2 70.1 60 44.6 42.2 63.0\n",
      "Pythia 6.7B* (Biderman et al., 2023) 63.5 76.3 64 61.1 61.3 35.2 37.2 56.9\n",
      "Pythia 12B* (Biderman et al., 2023) 67.7 76.6 67.3 63.8 63.9 34.8 38 58.9\n",
      "Fastchat T5* (Zheng et al., 2023) 81.5 64.6 46.3 61.8 49.3 33.3 39.4 53.7\n",
      "Fastchat Vicuña* 7B (Zheng et al., 2023) 76.6 77.2 70.7 67.3 53.5 41.2 40.8 61.0\n",
      "Fastchat Vicuña 13B* (Zheng et al., 2023) 81.5 76.8 73.3 66.7 57.4 42.7 43.6 63.1\n",
      "StableVicuña RLHF* (Stability-AI, 2023) 82.3 78.6 74.1 70.9 61 43.5 44.4 65.0\n",
      "StableLM Tuned* (Stability-AI, 2023) 62.5 71.2 53.6 54.8 52.4 31.1 33.4 51.3\n",
      "StableLM Base* (Stability-AI, 2023) 60.1 67.4 41.2 50.1 44.9 27 32 46.1\n",
      "Koala 13B* (Geng et al., 2023) 76.5 77.9 72.6 68.8 54.3 41 42.8 62.0\n",
      "Open Assistant Pythia 12B* 67.9 78 68.1 65 64.2 40.4 43.2 61.0\n",
      "Mosaic MPT7B (MosaicML-Team, 2023) 74.8 79.3 76.3 68.6 70 42.2 42.6 64.8\n",
      "Mosaic mpt-instruct (MosaicML-Team, 2023) 74.3 80.4 77.2 67.8 72.2 44.6 43 65.6\n",
      "Mosaic mpt-chat (MosaicML-Team, 2023) 77.1 78.2 74.5 67.5 69.4 43.3 44.2 64.9\n",
      "Wizard 7B (Xu et al., 2023) 78.4 77.2 69.9 66.5 56.8 40.5 42.6 61.7\n",
      "Wizard 7B Uncensored (Xu et al., 2023) 77.7 74.2 68 65.2 53.5 38.7 41.6 59.8\n",
      "Wizard 13B Uncensored (Xu et al., 2023) 78.4 75.5 72.1 69.5 57.5 40.4 44 62.5\n",
      "GPT4-x-Vicuna-13b (Nous-Research, 2023a) 81.3 75 75.2 65 58.7 43.9 43.6 63.2\n",
      "Falcon 7b (Almazrouei et al., 2023) 73.6 80.7 76.3 67.3 71 43.3 44.4 65.2\n",
      "Falcon 7b instruct (Almazrouei et al., 2023) 70.9 78.6 69.8 66.7 67.9 42.7 41.2 62.5\n",
      "text-davinci-003 88.1 83.8 83.4 75.8 83.9 63.9 51.0 75.7\n",
      "Table 1: Evaluations of all language models in the GPT4All ecosystem as of August 1, 2023. Code models are not\n",
      "included. OpenAI’s text-davinci-003 is included as a point of comparison. The best overall performing model in the included. OpenAI’s text-davinci-003 is included as a point of comparison. The best overall performing model in the\n",
      "GPT4All ecosystem, Nous-Hermes2, achieves over 92% of the average performance of text-davinci-003. Models\n",
      "marked with an asterisk were available in the ecosystem as of the release of GPT4All-Snoozy. Note that at release,\n",
      "GPT4All-Snoozy had the best average performance of any model in the ecosystem. Bolded numbers indicate the\n",
      "best performing model as of August 1, 2023. Figure 2: Comparison of the github start growth of GPT4All, Meta’s LLaMA, and Stanford’s Alpaca. We conjecture\n",
      "that GPT4All achieved and maintains faster ecosystem growth due to the focus on access, which allows more users\n",
      "to meaningfully participate.\n",
      "provides compressed versions of open source models\n",
      "for use on commodity hardware, stable and simple high\n",
      "level model APIs, and a GUI for no code model ex-\n",
      "perimentation. The project continues to increase in\n",
      "popularity, and as of August 1 2023, has garnered over\n",
      "50000 GitHub stars and over 5000 forks.\n",
      "GPT4All currently provides native support and\n",
      "benchmark data for over 35 models (see Figure 1), and\n",
      "includes several models co-developed with industry part-\n",
      "ners such as Replit and Hugging Face. GPT4All also\n",
      "provides high level model APIs in languages includ-\n",
      "ing Python, Typescript, Go, C#, and Java, among oth-\n",
      "ers. Furthermore, the GPT4All no code GUI currently\n",
      "supports the workflows of over 50000 monthly active\n",
      "users, with over 25% of users coming back to the tool\n",
      "every day of the week. (Note that all GPT4All user\n",
      "data is collected on an opt inbasis.) GPT4All has be-\n",
      "come the top language model integration in the popular\n",
      "open source AI orchestration library LangChain (Chase,\n",
      "2022), and powers many popular open source projects\n",
      "such as PrivateGPT (imartinez, 2023), Quiver (StanGi-\n",
      "rard, 2023), and MindsDB (MindsDB, 2023), among\n",
      "others. GPT4All is the 3rd fastest growing GitHub\n",
      "repository of all time (Leo, 2023), and is the 185th most\n",
      "popular repository on the platform, by star count.\n",
      "4 The Future of GPT4All\n",
      "In the future, we will continue to grow GPT4All, sup-\n",
      "porting it as the de facto solution for LLM accessibil-\n",
      "ity. Concretely, this means continuing to compress and\n",
      "distribute important open-source language models de-\n",
      "veloped by the community, as well as compressing and\n",
      "distributing increasingly multimodal AI models. Fur-\n",
      "thermore, we will expand the set of hardware devices\n",
      "that GPT4All models run on, so that GPT4All models\n",
      "“just work\" on any machine, whether it comes equipped\n",
      "with Apple Metal silicon, NVIDIA, AMD, or other edge-\n",
      "accelerated hardware. Overall, we envision a world\n",
      "where anyone, anywhere, with any machine, can access\n",
      "and contribute to the cutting edge of AI.\n",
      "Limitations\n",
      "By enabling access to large language models, the\n",
      "GPT4All project also inherits many of the ethical con-\n",
      "cerns associated with generative models. Principal\n",
      "among these is the concern that unfiltered language\n",
      "models like GPT4All enable malicious users to generate\n",
      "content that could be harmful and dangerous (e.g., in-\n",
      "structions on building bioweapons). While we recognize\n",
      "this risk, we also acknowledge the risk of concentrating\n",
      "this technology in the hands of a limited number of in-\n",
      "creasingly secretive research groups. We believe that\n",
      "the risk of focusing on the benefits of language model\n",
      "technology significantly outweighs the risk of misuse,\n",
      "and hence we prefer to make the technology as widely\n",
      "available as possible.\n",
      "Finally, we realize the challenge in assigning credit\n",
      "for large-scale open source initiatives. We make a first\n",
      "attempt at fair credit assignment by explicitly includ-\n",
      "ing the GPT4All open source developers as authors on\n",
      "this work, but recognize that this is insufficient fully\n",
      "characterize everyone involved in the GPT4All effort.\n",
      "Furthermore, we acknowledge the difficulty in citing\n",
      "open source works that do not necessarily have standard-\n",
      "ized citations, and do our best in this paper to provide\n",
      "URLs to projects whenever possible. We encourage\n",
      "further research in the area of open source credit as-\n",
      "signment, and hope to be able to support some of this\n",
      "research ourselves in the future. References\n",
      "Nomic AI. 2023. Atlas. https://atlas.nomic.ai/.\n",
      "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\n",
      "shamsi, Alessandro Cappelli, Ruxandra Cojocaru,\n",
      "Merouane Debbah, Etienne Goffinet, Daniel Hes-\n",
      "low, Julien Launay, Quentin Malartic, Badreddine\n",
      "Noune, Baptiste Pannier, and Guilherme Penedo.\n",
      "2023. Falcon-40B: an open large language model\n",
      "with state-of-the-art performance.\n",
      "Yuvanesh Anand, Zach Nussbaum, Brandon Duder-\n",
      "stadt, Benjamin Schmidt, and Andriy Mulyar. 2023.\n",
      "Gpt4all: Training an assistant-style chatbot with\n",
      "large scale data distillation from gpt-3.5-turbo.\n",
      "https://github.com/nomic-ai/gpt4all.\n",
      "BBC News. 2023. Chatgpt banned in italy over privacy\n",
      "concerns. BBC News.\n",
      "Stella Biderman, Hailey Schoelkopf, Quentin An-\n",
      "thony, Herbie Bradley, Kyle O’Brien, Eric Hal-\n",
      "lahan, Mohammad Aflah Khan, Shivanshu Puro-\n",
      "hit, USVSN Sai Prashanth, Edward Raff, Aviya\n",
      "Skowron, Lintang Sutawika, and Oskar van der Wal.\n",
      "2023. Pythia: A suite for analyzing large language\n",
      "models across training and scaling.\n",
      "Harrison Chase. 2022. langchain. https://github.\n",
      "com/langchain-ai/langchain.\n",
      "Mike Conover, Matt Hayes, Ankit Mathur, Xiangrui\n",
      "Meng, Jianwei Xie, Jun Wan, Ali Ghodsi, Patrick\n",
      "Wendell, and Matei Zaharia. 2023a. Hello dolly:\n",
      "Democratizing the magic of chatgpt with open mod-\n",
      "els.\n",
      "Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\n",
      "Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,\n",
      "Matei Zaharia, and Reynold Xin. 2023b. Free dolly:\n",
      "Introducing the world’s first truly open instruction-\n",
      "tuned llm.\n",
      "Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal-\n",
      "lace, Pieter Abbeel, Sergey Levine, and Dawn Song.\n",
      "2023. Koala: A dialogue model for academic re-\n",
      "search. Blog post.\n",
      "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\n",
      "Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\n",
      "Weizhu Chen. 2021. Lora: Low-rank adaptation of\n",
      "large language models.\n",
      "imartinez. 2023. privategpt. https://github.com/\n",
      "imartinez/privateGPT.\n",
      "Oscar Leo. 2023. GitHub: The Fastest Growing Repos-\n",
      "itories of All Time.\n",
      "Robert McMillan. 2023. A meta platforms leak put\n",
      "powerful ai in the hands of everyone. The Wall\n",
      "Street Journal.\n",
      "MindsDB. 2023. Mindsdb. https://github.com/\n",
      "mindsdb/mindsdb. GitHub repository.\n",
      "MosaicML-Team. 2023. Introducing mpt-7b: A new\n",
      "standard for open-source, commercially usable llms.\n",
      "Accessed: 2023-08-07.\n",
      "Nous-Research. 2023a. gpt4-x-vicuna-13b.\n",
      "https://huggingface.co/NousResearch/\n",
      "gpt4-x-vicuna-13b . Model on Hugging Face.\n",
      "Nous-Research. 2023b. Nous-hermes-13b.\n",
      "https://huggingface.co/NousResearch/\n",
      "Nous-Hermes-13b. Model on Hugging Face.\n",
      "Nous-Research. 2023c. Nous-hermes-llama-2-7b.\n",
      "https://huggingface.co/NousResearch/\n",
      "Nous-Hermes-llama-2-7b . Model on Hugging\n",
      "Face.\n",
      "Nous-Research. 2023d. Redmond-puffin-13b.\n",
      "https://huggingface.co/NousResearch/\n",
      "Redmond-Puffin-13B. Model on Hugging Face.\n",
      "OpenAI. 2023. Gpt-4 technical report.\n",
      "Victor Sanh, Albert Webson, Colin Raffel, Stephen H.\n",
      "Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\n",
      "Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\n",
      "Manan Dey, M Saiful Bari, Canwen Xu, Urmish\n",
      "Thakker, Shanya Sharma Sharma, Eliza Szczechla,\n",
      "Taewoon Kim, Gunjan Chhablani, Nihal Nayak,\n",
      "Debajyoti Datta, Jonathan Chang, Mike Tian-Jian\n",
      "Jiang, Han Wang, Matteo Manica, Sheng Shen,\n",
      "Zheng Xin Yong, Harshit Pandey, Rachel Bawden,\n",
      "Thomas Wang, Trishala Neeraj, Jos Rozen, Ab-\n",
      "heesht Sharma, Andrea Santilli, Thibault Fevry, Ja-\n",
      "son Alan Fries, Ryan Teehan, Stella Biderman, Leo\n",
      "Gao, Tali Bers, Thomas Wolf, and Alexander M.\n",
      "Rush. 2021. Multitask prompted training enables\n",
      "zero-shot task generalization.\n",
      "Stability-AI. 2023. Stablelm. https://github.com/\n",
      "Stability-AI/StableLM. GitHub repository.\n",
      "StanGirard. 2023. quivr. https://github.com/\n",
      "StanGirard/quivr. GitHub repository.\n",
      "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\n",
      "Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\n",
      "and Tatsunori B. Hashimoto. 2023. Stanford alpaca:\n",
      "An instruction-following llama model. https://\n",
      "github.com/tatsu-lab/stanford_alpaca.\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier and Tatsunori B. Hashimoto. 2023. Stanford alpaca:\n",
      "An instruction-following llama model. https://\n",
      "github.com/tatsu-lab/stanford_alpaca.\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\n",
      "Martinet, Marie-Anne Lachaux, Timothée Lacroix,\n",
      "Baptiste Rozière, Naman Goyal, Eric Hambro,\n",
      "Faisal Azhar, Aurelien Rodriguez, Armand Joulin,\n",
      "Edouard Grave, and Guillaume Lample. 2023.\n",
      "Llama: Open and efficient foundation language\n",
      "models.\n",
      "The Verge. 2023. Meta’s powerful ai language model\n",
      "has leaked online — what happens now? The Verge.\n",
      "James Vincent. 2023. As an ai generated language\n",
      "model: The phrase that shows how ai is polluting\n",
      "the web. The Verge. Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B:\n",
      "A 6 Billion Parameter Autoregressive Language\n",
      "Model. https://github.com/kingoflolz/\n",
      "mesh-transformer-jax.\n",
      "Eric J. Wang. 2023. alpaca-lora. https://github.\n",
      "com/tloen/alpaca-lora. GitHub repository.\n",
      "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\n",
      "Liu, Noah A. Smith, Daniel Khashabi, and Han-\n",
      "naneh Hajishirzi. 2023. Self-instruct: Aligning lan-\n",
      "guage models with self-generated instructions.\n",
      "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\n",
      "Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\n",
      "Jiang. 2023. Wizardlm: Empowering large language\n",
      "models to follow complex instructions.\n",
      "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\n",
      "Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\n",
      "Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\n",
      "Joseph E. Gonzalez, and Ion Stoica. 2023. Judging\n",
      "llm-as-a-judge with mt-bench and chatbot arena.\n",
      "Training dataset size: 4835\n",
      "Validation dataset size: 509\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Custom dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, context_length):\n",
    "        self.data = torch.tensor(data, dtype=torch.long)\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.context_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.context_length]\n",
    "        y = self.data[idx + 1:idx + self.context_length + 1]\n",
    "        return x, y\n",
    "\n",
    "# Tokenize the full text\n",
    "all_text = \" \".join(text_data)\n",
    "print(all_text)\n",
    "encoded_data = encode(all_text)\n",
    "\n",
    "# Split into train/validation sets\n",
    "context_length = 32\n",
    "train_size = int(0.9 * len(encoded_data))\n",
    "train_data = encoded_data[:train_size]\n",
    "val_data = encoded_data[train_size:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(train_data, context_length)\n",
    "val_dataset = TextDataset(val_data, context_length)\n",
    "\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ba81a3-4bc3-4984-9f1f-c8535ccf8fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([16, 32])\n",
      "Target batch shape: torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Inspect a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"Input batch shape:\", sample_batch[0].shape)\n",
    "print(\"Target batch shape:\", sample_batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ae3556-1918-4931-a75e-59bc9afe9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Define the Transformer model components\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embedding_dim, head_size)\n",
    "        self.query = nn.Linear(embedding_dim, head_size)\n",
    "        self.value = nn.Linear(embedding_dim, head_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(device)\n",
    "        wei = wei.masked_fill(mask, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        # Apply attention to values\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = hidden_dim // num_heads\n",
    "        self.attention = MultiHeadAttention(head_size, num_heads)\n",
    "        self.ff = FeedForward(hidden_dim)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Define the Small Language Model\n",
    "embedding_dim = 128\n",
    "num_heads = 12\n",
    "num_layers = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "class SmallLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(context_length, embedding_dim)\n",
    "        self.blocks = nn.Sequential(*[Block(embedding_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.ln = nn.LayerNorm(embedding_dim)\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.pos_embedding(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Apply Transformer block \n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -context_length:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "embedding_dim = 128\n",
    "num_heads = 12\n",
    "num_layers = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SmallLanguageModel().to(device)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b30ae2-56a6-44fa-8eab-9252c1745c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 1.4065\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_steps = 2000\n",
    "for step in range(num_steps):\n",
    "    for x, y in train_loader:\n",
    "        x, y = get_batch('train')\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        torch.save(model.state_dict(), f\"model_step_{step}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31671a-83ba-4ab7-9254-67a88031e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation\n",
    "start_prompt = \"Once upon a time\"\n",
    "encoded_prompt = torch.tensor([encode(start_prompt)], dtype=torch.long).to(device)\n",
    "generated = model.generate(encoded_prompt, max_new_tokens=50)\n",
    "print(\"Generated text:\", decode(generated[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
